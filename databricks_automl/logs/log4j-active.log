24/01/03 16:00:25 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:00:25 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:00:25 INFO AbstractConnector: Stopped Spark@a537b57{HTTP/1.1, (http/1.1)}{10.139.64.4:4041}
24/01/03 16:00:25 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4041
24/01/03 16:00:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:00:25 INFO MemoryStore: MemoryStore cleared
24/01/03 16:00:25 INFO BlockManager: BlockManager stopped
24/01/03 16:00:25 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:00:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:00:26 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:00:26 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:00:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-6a1108f7-61b2-4d4f-b85f-71de29ffed88
24/01/03 16:00:26 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-a7ea8e2d-ac67-4e0e-99c6-2f3b06689590/pyspark-5e189995-01b0-46ca-ba8d-7c96842410c4
24/01/03 16:00:26 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-a7ea8e2d-ac67-4e0e-99c6-2f3b06689590
24/01/03 16:01:35 INFO SecurityManager: Changing view acls to: root
24/01/03 16:01:37 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:01:37 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:01:37 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:01:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:01:38 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:01:38 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:01:38 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:01:38 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:01:38 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:01:38 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:01:40 INFO ResourceUtils: ==============================================================
24/01/03 16:01:40 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:01:40 INFO ResourceUtils: ==============================================================
24/01/03 16:01:40 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:01:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:01:40 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:01:40 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:01:40 INFO SecurityManager: Changing view acls to: root
24/01/03 16:01:40 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:01:40 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:01:40 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:01:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:01:42 INFO Utils: Successfully started service 'sparkDriver' on port 40939.
24/01/03 16:01:43 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:01:43 INFO SecurityManager: Changing view acls to: root
24/01/03 16:01:43 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:01:43 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:01:43 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:01:43 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:01:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:01:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:01:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:01:43 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-e4c31159-d8ed-434e-b8e7-6c417e06eae7
24/01/03 16:01:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:01:43 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:01:43 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:01:44 INFO log: Logging initialized @47661ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:01:44 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:01:44 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:01:44 INFO Server: Started @48466ms
24/01/03 16:01:45 INFO AbstractConnector: Started ServerConnector@6abf2110{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:01:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/01/03 16:01:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4231f3f{/,null,AVAILABLE,@Spark}
24/01/03 16:01:47 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:01:47 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:01:47 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:01:48 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:01:48 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:01:48 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:01:48 INFO Executor: Java version 1.8.0_382
24/01/03 16:01:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:01:48 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@407e8b56 for default.
24/01/03 16:01:48 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:01:48 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:01:48 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:01:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32849.
24/01/03 16:01:48 INFO NettyBlockTransferService: Server created on 10.139.64.4:32849
24/01/03 16:01:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:01:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 32849, None)
24/01/03 16:01:48 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:32849 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 32849, None)
24/01/03 16:01:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 32849, None)
24/01/03 16:01:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 32849, None)
24/01/03 16:01:56 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@4231f3f{/,null,STOPPED,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2e758942{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1c9ba7b6{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@26d07877{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@d15a9d2{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2dc89731{/stages,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@54350c5b{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3c66c6c3{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@168b9abf{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@365cd3d3{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@851a7b1{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@56195106{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3dc2f0c9{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@79dee2c3{/storage,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@36dcbff9{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@45c45f0b{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@180818ec{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@27ab7cd9{/environment,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@766527c6{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4898df2e{/executors,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2df8afca{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5e45d127{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2328e1bb{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@53134e4b{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@486435f8{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5944762b{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:01:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@352156bc{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:01:57 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3f29af09{/static,null,AVAILABLE,@Spark}
24/01/03 16:01:57 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7ebc9b9a{/,null,AVAILABLE,@Spark}
24/01/03 16:01:57 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@261065c2{/api,null,AVAILABLE,@Spark}
24/01/03 16:01:57 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2d00d72a{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:01:57 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3129019b{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:01:57 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a0a32cb{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:03:00 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:03:00 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:03:00 INFO AbstractConnector: Stopped Spark@6abf2110{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:03:00 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4040
24/01/03 16:03:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:03:01 INFO MemoryStore: MemoryStore cleared
24/01/03 16:03:01 INFO BlockManager: BlockManager stopped
24/01/03 16:03:01 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:03:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:03:01 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:03:01 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:03:01 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-d7cdc46d-70e5-4f36-a606-abce734836bb
24/01/03 16:03:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-bf853e9b-5637-4b87-8a70-69e96f05c1fa
24/01/03 16:03:02 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-d7cdc46d-70e5-4f36-a606-abce734836bb/pyspark-a7ea3c01-0e52-46bf-884e-1bda791571eb
24/01/03 16:05:35 INFO SecurityManager: Changing view acls to: root
24/01/03 16:05:37 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:05:37 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:05:37 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:05:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:05:38 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:05:38 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:05:38 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:05:39 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:05:39 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:05:39 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:05:40 INFO ResourceUtils: ==============================================================
24/01/03 16:05:40 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:05:40 INFO ResourceUtils: ==============================================================
24/01/03 16:05:40 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:05:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:05:40 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:05:40 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:05:40 INFO SecurityManager: Changing view acls to: root
24/01/03 16:05:40 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:05:40 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:05:40 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:05:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:05:43 INFO Utils: Successfully started service 'sparkDriver' on port 45927.
24/01/03 16:05:43 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:05:44 INFO SecurityManager: Changing view acls to: root
24/01/03 16:05:44 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:05:44 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:05:44 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:05:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:05:44 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:05:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:05:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:05:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:05:44 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-1ff4add6-b079-4261-bfff-6a3e8edc9a04
24/01/03 16:05:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:05:44 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:05:44 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:05:45 INFO log: Logging initialized @47128ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:05:46 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:05:46 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:05:46 INFO Server: Started @48386ms
24/01/03 16:05:47 INFO AbstractConnector: Started ServerConnector@7bc0f055{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:05:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/01/03 16:05:47 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1905d1f2{/,null,AVAILABLE,@Spark}
24/01/03 16:06:02 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:06:02 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:06:02 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:06:03 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:06:03 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:06:03 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:06:03 INFO Executor: Java version 1.8.0_382
24/01/03 16:06:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:06:03 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@706da32 for default.
24/01/03 16:06:03 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:06:03 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:06:03 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:06:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34221.
24/01/03 16:06:03 INFO NettyBlockTransferService: Server created on 10.139.64.4:34221
24/01/03 16:06:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:06:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 34221, None)
24/01/03 16:06:03 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:34221 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 34221, None)
24/01/03 16:06:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 34221, None)
24/01/03 16:06:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 34221, None)
24/01/03 16:06:09 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@1905d1f2{/,null,STOPPED,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@14a4030a{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7bffd089{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@112cc2df{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@15e2879{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1041b4d0{/stages,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6564ff60{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@65dd5e3c{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2e940b3e{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@743ed13{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@34fb6db8{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@275730a6{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@308fc111{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3272b404{/storage,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@142abf05{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6c278e84{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6cc682cb{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@61c10c1e{/environment,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3a088e4e{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@751cba6c{/executors,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6dbe02f7{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@462e3068{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1cc624ea{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@748ac427{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@49987254{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@28c8ded0{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6185fb04{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@9508889{/static,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@88b51bf{/,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@467d936a{/api,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@eecff5d{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:06:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@bfcdad3{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:06:10 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@488bf4da{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:06:33 INFO SecurityManager: Changing view acls to: root
24/01/03 16:06:36 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:06:36 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:06:36 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:06:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:06:34 INFO SecurityManager: Changing view acls to: root
24/01/03 16:06:37 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:06:37 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:06:37 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:06:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:06:37 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:06:37 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:06:37 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:06:37 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:06:37 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:06:37 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:06:37 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:06:37 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:06:37 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:06:37 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:06:37 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:06:37 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:06:37 INFO ResourceUtils: ==============================================================
24/01/03 16:06:37 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:06:37 INFO ResourceUtils: ==============================================================
24/01/03 16:06:37 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:06:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:06:38 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:06:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:06:38 INFO SecurityManager: Changing view acls to: root
24/01/03 16:06:38 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:06:38 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:06:38 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:06:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:06:38 INFO ResourceUtils: ==============================================================
24/01/03 16:06:38 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:06:38 INFO ResourceUtils: ==============================================================
24/01/03 16:06:38 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:06:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:06:38 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:06:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:06:38 INFO SecurityManager: Changing view acls to: root
24/01/03 16:06:38 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:06:38 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:06:38 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:06:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:06:40 INFO Utils: Successfully started service 'sparkDriver' on port 37251.
24/01/03 16:06:40 INFO Utils: Successfully started service 'sparkDriver' on port 45143.
24/01/03 16:06:40 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:06:40 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:06:40 INFO SecurityManager: Changing view acls to: root
24/01/03 16:06:40 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:06:40 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:06:40 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:06:40 INFO SecurityManager: Changing view acls to: root
24/01/03 16:06:40 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:06:40 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:06:40 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:06:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:06:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:06:40 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:06:40 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:06:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:06:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:06:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:06:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:06:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:06:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:06:40 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-2df6fa7f-13ca-4382-bdfe-34fcded3ce59
24/01/03 16:06:40 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-15e76e0e-e832-451f-9962-0e66973d7c8e
24/01/03 16:06:40 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:06:40 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:06:40 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:06:40 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:06:40 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:06:40 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:06:41 INFO log: Logging initialized @35489ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:06:41 INFO log: Logging initialized @35922ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:06:41 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:06:41 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:06:41 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:06:41 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:06:41 INFO Server: Started @36555ms
24/01/03 16:06:41 INFO Server: Started @36226ms
24/01/03 16:06:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/01/03 16:06:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/01/03 16:06:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
24/01/03 16:06:42 INFO AbstractConnector: Started ServerConnector@3afdd31c{HTTP/1.1, (http/1.1)}{10.139.64.4:4041}
24/01/03 16:06:42 INFO Utils: Successfully started service 'SparkUI' on port 4041.
24/01/03 16:06:42 INFO AbstractConnector: Started ServerConnector@29373362{HTTP/1.1, (http/1.1)}{10.139.64.4:4042}
24/01/03 16:06:42 INFO Utils: Successfully started service 'SparkUI' on port 4042.
24/01/03 16:06:42 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@c237a06{/,null,AVAILABLE,@Spark}
24/01/03 16:06:42 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@160dafd{/,null,AVAILABLE,@Spark}
24/01/03 16:06:45 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:06:45 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:06:45 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:06:45 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:06:45 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:06:45 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:06:46 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:06:46 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:06:46 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:06:46 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:06:46 INFO Executor: Java version 1.8.0_382
24/01/03 16:06:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:06:46 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1d6827bf for default.
24/01/03 16:06:46 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:06:46 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:06:46 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:06:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45541.
24/01/03 16:06:46 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:06:46 INFO NettyBlockTransferService: Server created on 10.139.64.4:45541
24/01/03 16:06:46 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:06:46 INFO Executor: Java version 1.8.0_382
24/01/03 16:06:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:06:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 45541, None)
24/01/03 16:06:46 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:45541 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 45541, None)
24/01/03 16:06:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 45541, None)
24/01/03 16:06:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 45541, None)
24/01/03 16:06:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:06:46 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@57fac4e for default.
24/01/03 16:06:46 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:06:46 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:06:46 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:06:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39821.
24/01/03 16:06:46 INFO NettyBlockTransferService: Server created on 10.139.64.4:39821
24/01/03 16:06:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:06:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 39821, None)
24/01/03 16:06:46 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:39821 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 39821, None)
24/01/03 16:06:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 39821, None)
24/01/03 16:06:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 39821, None)
24/01/03 16:06:51 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@160dafd{/,null,STOPPED,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@8c52b68{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@24894e59{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7cc04abc{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@55a68da4{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@32556293{/stages,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@68d6f1e4{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7d0f5e5a{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@21869e00{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a085ad6{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1a80cc18{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4efd544d{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@445f071d{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7d5741ae{/storage,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6686a35{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@79ae9ca2{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1968818e{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@35bbce51{/environment,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@17e67690{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2c1ae372{/executors,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3dd3115d{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@42b19b29{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@536ce648{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7b970c8e{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3b5883e2{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@406f05f3{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@232e8ddd{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@c237a06{/,null,STOPPED,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@297fb48d{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@d99d62c{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7598d989{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@66a75c37{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@290e947e{/stages,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5906c4d6{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5d5e4263{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6b44c6b{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@64a519ed{/static,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@13f9518c{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@63d4e93{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@61c51418{/,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@396575a7{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@395c2bc{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3847c6c1{/storage,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@78b761c3{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3e2e6eaf{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@52e9fbf0{/api,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7e025238{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@450e6367{/environment,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@21e7f28b{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7b9b5700{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@50595138{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2fd5a72b{/executors,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3b0f1eb1{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@17e89d3f{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@63b31f64{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5f46b781{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2dd1e77c{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@251d10a0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6553a9d1{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@d4d6f83{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@684cbd92{/static,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1d77d3b0{/,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@200c84e7{/api,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@71b51e8e{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5ec1e35c{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:06:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3c65d7bd{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:07:16 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:07:16 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:07:16 INFO AbstractConnector: Stopped Spark@7bc0f055{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:07:16 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4040
24/01/03 16:07:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:07:16 INFO MemoryStore: MemoryStore cleared
24/01/03 16:07:16 INFO BlockManager: BlockManager stopped
24/01/03 16:07:16 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:07:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:07:16 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:07:17 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:07:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-ba96d78f-8492-44bb-b127-26e6f11693e5
24/01/03 16:07:17 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-4dd0ff30-6502-450c-897a-73db845745e7/pyspark-3689b201-af24-4aae-ae38-a73e0f7c8ee4
24/01/03 16:07:17 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-4dd0ff30-6502-450c-897a-73db845745e7
24/01/03 16:08:30 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:08:30 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:08:30 INFO AbstractConnector: Stopped Spark@29373362{HTTP/1.1, (http/1.1)}{10.139.64.4:4042}
24/01/03 16:08:30 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4042
24/01/03 16:08:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:08:31 INFO MemoryStore: MemoryStore cleared
24/01/03 16:08:31 INFO BlockManager: BlockManager stopped
24/01/03 16:08:31 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:08:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:08:31 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:08:32 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:08:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-10d952a2-0fd8-4aac-9887-da6bee96432b
24/01/03 16:08:32 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-449fdf5b-08d0-4b0c-a4f0-6bea5d34d0ea
24/01/03 16:08:32 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:08:32 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:08:32 INFO AbstractConnector: Stopped Spark@3afdd31c{HTTP/1.1, (http/1.1)}{10.139.64.4:4041}
24/01/03 16:08:32 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4041
24/01/03 16:08:32 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-449fdf5b-08d0-4b0c-a4f0-6bea5d34d0ea/pyspark-d15ffff4-c239-443c-ac64-7c64a61618a1
24/01/03 16:08:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:08:33 INFO MemoryStore: MemoryStore cleared
24/01/03 16:08:33 INFO BlockManager: BlockManager stopped
24/01/03 16:08:33 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:08:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:08:33 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:08:33 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:08:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-24b047f5-917a-4fc4-93be-c7fb68438e12
24/01/03 16:08:33 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-06be3fca-aef7-4ad7-89c9-25fa821573be
24/01/03 16:08:33 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-06be3fca-aef7-4ad7-89c9-25fa821573be/pyspark-d57b4a42-bf8c-460f-9fea-68b2c82b47e8
24/01/03 16:11:12 INFO SecurityManager: Changing view acls to: root
24/01/03 16:11:17 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:11:17 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:11:17 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:11:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:11:18 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:11:18 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:11:18 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:11:18 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:11:18 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:11:18 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:11:20 INFO ResourceUtils: ==============================================================
24/01/03 16:11:20 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:11:20 INFO ResourceUtils: ==============================================================
24/01/03 16:11:20 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:11:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:11:21 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:11:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:11:21 INFO SecurityManager: Changing view acls to: root
24/01/03 16:11:21 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:11:21 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:11:21 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:11:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:11:25 INFO Utils: Successfully started service 'sparkDriver' on port 38913.
24/01/03 16:11:26 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:11:26 INFO SecurityManager: Changing view acls to: root
24/01/03 16:11:26 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:11:26 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:11:26 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:11:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:11:26 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:11:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:11:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:11:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:11:27 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-b5c1b6c4-54bc-4719-bea0-ee0522aebf7e
24/01/03 16:11:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:11:27 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:11:27 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:11:28 INFO log: Logging initialized @67489ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:11:29 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:11:29 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:11:30 INFO Server: Started @69268ms
24/01/03 16:11:30 INFO AbstractConnector: Started ServerConnector@55ea597e{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:11:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/01/03 16:11:31 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1dfda1c5{/,null,AVAILABLE,@Spark}
24/01/03 16:11:36 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:11:36 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:11:36 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:11:37 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:11:38 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:11:38 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:11:38 INFO Executor: Java version 1.8.0_382
24/01/03 16:11:38 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:11:38 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@726b0564 for default.
24/01/03 16:11:38 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:11:38 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:11:38 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:11:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41443.
24/01/03 16:11:39 INFO NettyBlockTransferService: Server created on 10.139.64.4:41443
24/01/03 16:11:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:11:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 41443, None)
24/01/03 16:11:39 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:41443 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 41443, None)
24/01/03 16:11:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 41443, None)
24/01/03 16:11:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 41443, None)
24/01/03 16:11:51 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@1dfda1c5{/,null,STOPPED,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5e0384e4{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3989c64b{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@618d4a8b{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@19c3ff23{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6b8383d6{/stages,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@70780cde{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@dcf3df3{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2feadd6c{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@682f4c95{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@155b6b39{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@45e95f73{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@79a7baef{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@260d4863{/storage,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@dd12d66{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6ca9c1bf{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@da412bb{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@512be4f6{/environment,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@20d69554{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3659043d{/executors,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7c597f12{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@60ba8be0{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@36a7a0da{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7cd3263f{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7936c13b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@46631fa5{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1bebcf04{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4774dc02{/static,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@671d2429{/,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7f28a569{/api,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4105a211{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7dbc3174{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:11:51 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@576ac90e{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:12:40 INFO SecurityManager: Changing view acls to: root
24/01/03 16:12:44 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:12:44 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:12:44 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:12:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:12:44 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:12:44 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:12:44 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:12:44 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:12:44 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:12:44 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:12:45 INFO ResourceUtils: ==============================================================
24/01/03 16:12:45 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:12:45 INFO ResourceUtils: ==============================================================
24/01/03 16:12:45 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:12:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:12:45 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:12:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:12:45 INFO SecurityManager: Changing view acls to: root
24/01/03 16:12:45 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:12:45 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:12:45 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:12:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:12:47 INFO Utils: Successfully started service 'sparkDriver' on port 34011.
24/01/03 16:12:47 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:12:47 INFO SecurityManager: Changing view acls to: root
24/01/03 16:12:47 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:12:47 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:12:47 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:12:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:12:47 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:12:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:12:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:12:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:12:47 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-7aa3d6cb-12b2-4a2d-9f7f-d05ba78a0735
24/01/03 16:12:47 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:12:47 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:12:47 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:12:48 INFO log: Logging initialized @33769ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:12:48 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:12:48 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:12:48 INFO Server: Started @34340ms
24/01/03 16:12:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/01/03 16:12:48 INFO AbstractConnector: Started ServerConnector@4fa97dcd{HTTP/1.1, (http/1.1)}{10.139.64.4:4041}
24/01/03 16:12:49 INFO Utils: Successfully started service 'SparkUI' on port 4041.
24/01/03 16:12:49 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7e4cfaa3{/,null,AVAILABLE,@Spark}
24/01/03 16:12:50 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:12:50 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:12:50 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:12:51 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:12:51 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:12:51 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:12:51 INFO Executor: Java version 1.8.0_382
24/01/03 16:12:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:12:51 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@23c9dbde for default.
24/01/03 16:12:51 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:12:51 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:12:51 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:12:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33167.
24/01/03 16:12:51 INFO NettyBlockTransferService: Server created on 10.139.64.4:33167
24/01/03 16:12:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:12:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 33167, None)
24/01/03 16:12:52 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:33167 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 33167, None)
24/01/03 16:12:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 33167, None)
24/01/03 16:12:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 33167, None)
24/01/03 16:12:55 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:12:55 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:12:55 INFO AbstractConnector: Stopped Spark@55ea597e{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:12:55 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4040
24/01/03 16:12:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:12:55 INFO MemoryStore: MemoryStore cleared
24/01/03 16:12:55 INFO BlockManager: BlockManager stopped
24/01/03 16:12:55 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:12:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:12:56 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:12:56 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:12:56 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-6487cbc4-244c-4263-a820-040318467964/pyspark-a89365b3-be44-4f13-afb7-4d3895c0fb23
24/01/03 16:12:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-a389ce7a-b101-40f1-a4d7-771acbfecdb5
24/01/03 16:12:56 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-6487cbc4-244c-4263-a820-040318467964
24/01/03 16:13:01 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@7e4cfaa3{/,null,STOPPED,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@688a758c{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2b95d848{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@53672900{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@678368e{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6e7764c4{/stages,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@50553d89{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@9b6e7df{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7cacfe85{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@384e386{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@21ef8c80{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2bd08c16{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2c5cb562{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6a73db76{/storage,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7812133{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@546922ee{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6f34890e{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7d42e3f8{/environment,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4296cb95{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4674c097{/executors,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@31924d33{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@60ae45b6{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@619bcc91{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@12c4d65{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@63a37da4{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7a8a0685{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4072b47d{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@21d1dde{/static,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@63c649d8{/,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2bbc2040{/api,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@31f230bc{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@617720c1{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:13:01 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6b42d863{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:13:41 INFO SecurityManager: Changing view acls to: root
24/01/03 16:13:45 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:13:45 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:13:45 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:13:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:13:46 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:13:46 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:13:46 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:13:46 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:13:46 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:13:46 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:13:48 INFO ResourceUtils: ==============================================================
24/01/03 16:13:48 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:13:48 INFO ResourceUtils: ==============================================================
24/01/03 16:13:48 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:13:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:13:48 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:13:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:13:48 INFO SecurityManager: Changing view acls to: root
24/01/03 16:13:48 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:13:48 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:13:48 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:13:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:13:51 INFO Utils: Successfully started service 'sparkDriver' on port 34657.
24/01/03 16:13:51 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:13:51 INFO SecurityManager: Changing view acls to: root
24/01/03 16:13:51 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:13:51 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:13:51 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:13:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:13:52 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:13:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:13:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:13:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:13:52 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-c452bf8a-cb1f-4987-a949-46d3807b1ef0
24/01/03 16:13:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:13:52 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:13:53 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:13:53 INFO log: Logging initialized @48253ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:13:54 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:13:55 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:13:55 INFO Server: Started @49964ms
24/01/03 16:13:55 INFO AbstractConnector: Started ServerConnector@6aebc947{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:13:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/01/03 16:13:56 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6c7ad3fe{/,null,AVAILABLE,@Spark}
24/01/03 16:13:52 INFO SecurityManager: Changing view acls to: root
24/01/03 16:13:56 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:13:56 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:13:56 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:13:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:13:57 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:13:57 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:13:57 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:13:57 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:13:57 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:13:57 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:13:58 INFO ResourceUtils: ==============================================================
24/01/03 16:13:58 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:13:58 INFO ResourceUtils: ==============================================================
24/01/03 16:13:58 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:13:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:13:58 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:13:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:13:59 INFO SecurityManager: Changing view acls to: root
24/01/03 16:13:59 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:13:59 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:13:59 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:13:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:13:59 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:13:59 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:13:59 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:14:01 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:14:01 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:14:01 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:14:01 INFO Executor: Java version 1.8.0_382
24/01/03 16:14:01 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:14:01 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4ca7abbc for default.
24/01/03 16:14:01 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:14:02 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:14:02 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:14:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44063.
24/01/03 16:14:02 INFO NettyBlockTransferService: Server created on 10.139.64.4:44063
24/01/03 16:14:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:14:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 44063, None)
24/01/03 16:14:02 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:44063 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 44063, None)
24/01/03 16:14:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 44063, None)
24/01/03 16:14:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 44063, None)
24/01/03 16:14:02 INFO Utils: Successfully started service 'sparkDriver' on port 42145.
24/01/03 16:14:03 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:14:03 INFO SecurityManager: Changing view acls to: root
24/01/03 16:14:03 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:14:03 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:14:03 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:14:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:14:03 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:14:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:14:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:14:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:14:03 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-a30b7828-13c2-443b-aef6-68ec6a694bd0
24/01/03 16:14:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:14:04 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:14:04 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:14:05 INFO log: Logging initialized @49241ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:14:06 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:14:06 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:14:06 INFO Server: Started @50853ms
24/01/03 16:14:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/01/03 16:14:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
24/01/03 16:14:07 INFO AbstractConnector: Started ServerConnector@88df8cd{HTTP/1.1, (http/1.1)}{10.139.64.4:4042}
24/01/03 16:14:07 INFO Utils: Successfully started service 'SparkUI' on port 4042.
24/01/03 16:14:07 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@19a7bdc0{/,null,AVAILABLE,@Spark}
24/01/03 16:14:11 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:14:11 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:14:11 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:14:12 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:14:12 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:14:12 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:14:12 INFO Executor: Java version 1.8.0_382
24/01/03 16:14:12 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:14:12 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5e2ca565 for default.
24/01/03 16:14:12 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:14:12 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:14:12 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:14:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33751.
24/01/03 16:14:12 INFO NettyBlockTransferService: Server created on 10.139.64.4:33751
24/01/03 16:14:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:14:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 33751, None)
24/01/03 16:14:12 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@6c7ad3fe{/,null,STOPPED,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d4ddecc{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@205eadeb{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:33751 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 33751, None)
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a6deb0d{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@21ef3f51{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@793a2f41{/stages,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@771afae2{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6282c664{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@9451f69{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@409b8888{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@129311a5{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4c00ddef{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@12bba5a2{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@169658f6{/storage,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3d8d881b{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5d461a71{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5bf3f68c{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@523ed8c4{/environment,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 33751, None)
24/01/03 16:14:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 33751, None)
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@67f70647{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4aa227a5{/executors,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@10506a09{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@54a7df09{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5ca1a009{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2310cd5e{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@29d75826{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6dabfd39{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:14:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3e3fdcb0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:14:13 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@61cc4b5b{/static,null,AVAILABLE,@Spark}
24/01/03 16:14:13 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1760d26c{/,null,AVAILABLE,@Spark}
24/01/03 16:14:13 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@22530219{/api,null,AVAILABLE,@Spark}
24/01/03 16:14:13 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@55bc9234{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:14:13 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@251fd065{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:14:13 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6dfd833c{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@19a7bdc0{/,null,STOPPED,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@12d9ed5a{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@44d78855{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@265cb222{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1d2032b5{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@347e6a35{/stages,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7e2c11aa{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1b7e7b61{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1b704155{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7a2c855c{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d3a2551{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6bd36d25{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6e5cd513{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@33754952{/storage,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6abde571{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4de8cff7{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@20d113e1{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@eb92db1{/environment,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@144d1a18{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@783c4210{/executors,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@62539194{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3eabc3c2{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@489c7730{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@65d568a0{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@14466979{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5f6d6788{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@217eac9{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3ba7f807{/static,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@69cd9a80{/,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@62a88933{/api,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@9b8e89{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@592af328{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:14:27 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6820851{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:15:50 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:15:50 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:15:50 INFO AbstractConnector: Stopped Spark@6aebc947{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:15:50 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4040
24/01/03 16:15:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:15:50 INFO MemoryStore: MemoryStore cleared
24/01/03 16:15:50 INFO BlockManager: BlockManager stopped
24/01/03 16:15:50 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:15:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:15:50 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:15:51 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:15:51 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-95473ec0-3c43-4492-aea7-dc38a3f4ba08/pyspark-5ca11779-0d8d-4562-868a-94605344f5b3
24/01/03 16:15:51 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-95473ec0-3c43-4492-aea7-dc38a3f4ba08
24/01/03 16:15:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-3dcd59c7-692b-4c02-99df-3b47e96d407f
24/01/03 16:15:58 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:15:58 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:15:58 INFO AbstractConnector: Stopped Spark@88df8cd{HTTP/1.1, (http/1.1)}{10.139.64.4:4042}
24/01/03 16:15:58 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4042
24/01/03 16:15:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:15:58 INFO MemoryStore: MemoryStore cleared
24/01/03 16:15:58 INFO BlockManager: BlockManager stopped
24/01/03 16:15:58 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:15:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:15:58 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:15:58 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:15:58 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-64eb05d3-d196-46cf-a2d6-096718c796aa/pyspark-7e6ece48-ad96-4609-bf6c-212f5edf137f
24/01/03 16:15:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-b716e941-cbbd-4247-8173-7d304e20cf87
24/01/03 16:15:59 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-64eb05d3-d196-46cf-a2d6-096718c796aa
24/01/03 16:16:03 INFO SecurityManager: Changing view acls to: root
24/01/03 16:16:06 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:16:06 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:16:06 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:16:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:16:06 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:16:06 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:16:06 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:16:06 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:16:06 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:16:06 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:16:07 INFO ResourceUtils: ==============================================================
24/01/03 16:16:07 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:16:07 INFO ResourceUtils: ==============================================================
24/01/03 16:16:07 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:16:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:16:07 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:16:08 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:16:08 INFO SecurityManager: Changing view acls to: root
24/01/03 16:16:08 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:16:08 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:16:08 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:16:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:16:09 INFO Utils: Successfully started service 'sparkDriver' on port 33193.
24/01/03 16:16:10 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:16:10 INFO SecurityManager: Changing view acls to: root
24/01/03 16:16:10 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:16:10 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:16:10 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:16:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:16:10 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:16:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:16:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:16:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:16:10 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-9ae2d277-a433-4c36-b7b6-5fb2a28900e6
24/01/03 16:16:10 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:16:10 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:16:11 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:16:11 INFO log: Logging initialized @42056ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:16:12 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:16:12 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:16:12 INFO Server: Started @42934ms
24/01/03 16:16:12 INFO AbstractConnector: Started ServerConnector@5ae5a0ac{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:16:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/01/03 16:16:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5519b957{/,null,AVAILABLE,@Spark}
24/01/03 16:16:14 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:16:14 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:16:14 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:16:15 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:16:15 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:16:15 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:16:15 INFO Executor: Java version 1.8.0_382
24/01/03 16:16:15 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:16:15 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7661642d for default.
24/01/03 16:16:15 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:16:15 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:16:15 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:16:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37043.
24/01/03 16:16:15 INFO NettyBlockTransferService: Server created on 10.139.64.4:37043
24/01/03 16:16:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:16:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 37043, None)
24/01/03 16:16:15 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:37043 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 37043, None)
24/01/03 16:16:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 37043, None)
24/01/03 16:16:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 37043, None)
24/01/03 16:16:20 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@5519b957{/,null,STOPPED,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@380308bc{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@511f5b86{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@e0e3771{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@95941b6{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4f9c2ae0{/stages,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1d0e7fac{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4c9090e2{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4de6b6e4{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4cbd1dde{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6f6f4c54{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5bbb49c9{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d08ddb3{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@74e8468a{/storage,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@26570a20{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@561df4{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5ecc4838{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3e244e7b{/environment,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@60c3ef05{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@181ea3ee{/executors,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2cea04c8{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6ee8a8ab{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@32c59351{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@39b05338{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@41f2642b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@581539cf{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4c3db219{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6237f3d8{/static,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@38e569f8{/,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5eef8110{/api,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@79e9ff9d{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3f2bf236{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:16:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@44b7284e{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:16:52 INFO SecurityManager: Changing view acls to: root
24/01/03 16:16:56 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:16:56 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:16:51 INFO SecurityManager: Changing view acls to: root
24/01/03 16:16:56 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:16:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:16:56 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:16:56 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:16:56 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:16:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:16:56 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:16:56 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:16:56 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:16:56 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:16:56 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:16:57 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:16:57 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:16:57 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:16:57 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:16:57 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:16:57 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:16:57 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:16:58 INFO ResourceUtils: ==============================================================
24/01/03 16:16:58 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:16:58 INFO ResourceUtils: ==============================================================
24/01/03 16:16:58 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:16:58 INFO ResourceUtils: ==============================================================
24/01/03 16:16:58 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:16:58 INFO ResourceUtils: ==============================================================
24/01/03 16:16:58 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:16:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:16:58 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:16:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:16:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:16:58 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:16:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:16:58 INFO SecurityManager: Changing view acls to: root
24/01/03 16:16:58 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:16:58 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:16:58 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:16:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:16:58 INFO SecurityManager: Changing view acls to: root
24/01/03 16:16:58 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:16:58 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:16:58 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:16:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:17:00 INFO Utils: Successfully started service 'sparkDriver' on port 44091.
24/01/03 16:17:00 INFO Utils: Successfully started service 'sparkDriver' on port 34915.
24/01/03 16:17:00 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:17:00 INFO SecurityManager: Changing view acls to: root
24/01/03 16:17:00 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:17:00 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:17:00 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:17:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:17:00 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:17:00 INFO SecurityManager: Changing view acls to: root
24/01/03 16:17:00 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:17:00 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:17:00 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:17:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:17:00 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:17:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:17:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:17:00 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:17:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:17:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:17:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:17:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:17:00 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-ca1e0806-0b61-47fd-8b13-d27e9969711d
24/01/03 16:17:00 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-7d64424f-e7b7-4209-af1a-79c97b520998
24/01/03 16:17:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:17:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:17:01 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:17:01 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:17:01 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:17:01 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:17:01 INFO log: Logging initialized @37832ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:17:01 INFO log: Logging initialized @35425ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:17:02 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:17:02 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:17:02 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:17:02 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:17:02 INFO Server: Started @36212ms
24/01/03 16:17:02 INFO Server: Started @38794ms
24/01/03 16:17:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/01/03 16:17:02 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
24/01/03 16:17:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/01/03 16:17:02 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
24/01/03 16:17:02 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
24/01/03 16:17:02 INFO AbstractConnector: Started ServerConnector@22fac146{HTTP/1.1, (http/1.1)}{10.139.64.4:4042}
24/01/03 16:17:02 INFO Utils: Successfully started service 'SparkUI' on port 4042.
24/01/03 16:17:02 INFO AbstractConnector: Started ServerConnector@71f44018{HTTP/1.1, (http/1.1)}{10.139.64.4:4043}
24/01/03 16:17:02 INFO Utils: Successfully started service 'SparkUI' on port 4043.
24/01/03 16:17:03 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5ca18e57{/,null,AVAILABLE,@Spark}
24/01/03 16:17:03 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6dd7ae12{/,null,AVAILABLE,@Spark}
24/01/03 16:17:07 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:17:07 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:17:07 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:17:07 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:17:07 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:17:07 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:17:07 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:17:07 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:17:08 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:17:08 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:17:08 INFO Executor: Java version 1.8.0_382
24/01/03 16:17:08 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:17:08 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:17:08 INFO Executor: Java version 1.8.0_382
24/01/03 16:17:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:17:08 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@c1a67a4 for default.
24/01/03 16:17:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:17:08 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@47a3da40 for default.
24/01/03 16:17:08 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:17:08 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:17:08 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:17:08 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:17:08 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:17:08 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:17:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45445.
24/01/03 16:17:08 INFO NettyBlockTransferService: Server created on 10.139.64.4:45445
24/01/03 16:17:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:17:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40145.
24/01/03 16:17:08 INFO NettyBlockTransferService: Server created on 10.139.64.4:40145
24/01/03 16:17:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 45445, None)
24/01/03 16:17:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:17:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 40145, None)
24/01/03 16:17:08 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:45445 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 45445, None)
24/01/03 16:17:08 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:40145 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 40145, None)
24/01/03 16:17:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 40145, None)
24/01/03 16:17:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 40145, None)
24/01/03 16:17:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 45445, None)
24/01/03 16:17:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 45445, None)
24/01/03 16:17:16 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@5ca18e57{/,null,STOPPED,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@6dd7ae12{/,null,STOPPED,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@143f7bf1{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@f33cb88{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@18d61676{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@64929b13{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@36082812{/stages,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@85fc229{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5489d75f{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d7014f5{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6c7cafbf{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@9a8f16f{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1ec5e81a{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@78659435{/stages,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@64f76f38{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@54c0707b{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@39eb6e00{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@e05475e{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5dc82d2a{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@70dbf20a{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@33b70430{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f0add86{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@64a3e995{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@32d81eaa{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b470ca9{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4251b3a9{/storage,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43dfa8da{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@39c51a45{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3180dd0e{/storage,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1ddab8df{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@14a28a25{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@52d995d6{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2a057ec7{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@acbca42{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@436d3a46{/environment,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@17533857{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@421f8784{/environment,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@431e73ac{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3db1eeb{/executors,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3d778985{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3ae60c74{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@29ceae9e{/executors,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4c326774{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2c91a730{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@79e2bbda{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@a76b367{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@33e89e2f{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@639a4457{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@61916aeb{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4d88f4e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@261e8a2b{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@26b471bf{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2b7f291f{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:17:16 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@fca021f{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@31ffbeea{/static,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5b4041cb{/,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@eadce39{/static,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6e25f944{/,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@362f6ecd{/api,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@70b215de{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1391bc81{/api,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@563dccfd{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@67d844aa{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2e63f715{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7848c5a7{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:17:17 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@594d28e4{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:17:21 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:17:21 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:17:21 INFO AbstractConnector: Stopped Spark@5ae5a0ac{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:17:21 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4040
24/01/03 16:17:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:17:21 INFO MemoryStore: MemoryStore cleared
24/01/03 16:17:21 INFO BlockManager: BlockManager stopped
24/01/03 16:17:21 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:17:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:17:21 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:17:21 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:17:21 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-25cdfba2-fcc7-493d-94ac-ae15e25c7201
24/01/03 16:17:22 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-25cdfba2-fcc7-493d-94ac-ae15e25c7201/pyspark-fcbc5117-498d-4b1c-98eb-2cf93e516f59
24/01/03 16:17:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-850e8b52-1629-4b35-97e5-4b98dc6163af
24/01/03 16:18:11 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:18:11 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:18:11 INFO AbstractConnector: Stopped Spark@22fac146{HTTP/1.1, (http/1.1)}{10.139.64.4:4042}
24/01/03 16:18:11 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4042
24/01/03 16:18:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:18:11 INFO MemoryStore: MemoryStore cleared
24/01/03 16:18:11 INFO BlockManager: BlockManager stopped
24/01/03 16:18:11 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:18:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:18:12 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:18:12 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:18:12 INFO AbstractConnector: Stopped Spark@71f44018{HTTP/1.1, (http/1.1)}{10.139.64.4:4043}
24/01/03 16:18:12 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:18:12 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4043
24/01/03 16:18:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:18:12 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:18:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-15b851e6-a1f1-4f70-b917-5ba2828ba2af
24/01/03 16:18:12 INFO MemoryStore: MemoryStore cleared
24/01/03 16:18:12 INFO BlockManager: BlockManager stopped
24/01/03 16:18:12 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:18:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:18:12 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:18:12 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-335b27f5-3145-479c-9bb0-1a918c24b684
24/01/03 16:18:12 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:18:12 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-1881fd06-5e78-4d0a-8cb1-aec8adb1c450
24/01/03 16:18:13 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-335b27f5-3145-479c-9bb0-1a918c24b684/pyspark-028d2d59-57b2-4509-b34a-1f5941fc4351
24/01/03 16:18:13 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-1881fd06-5e78-4d0a-8cb1-aec8adb1c450/pyspark-1fce7f95-a9d3-4068-8c67-ca0e33403155
24/01/03 16:18:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-39995477-d91a-4223-b2a9-c832d3fdca00
24/01/03 16:24:26 INFO SecurityManager: Changing view acls to: root
24/01/03 16:24:31 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:24:31 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:24:31 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:24:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:24:33 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:24:33 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:24:33 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:24:33 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:24:33 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:24:33 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:24:35 INFO ResourceUtils: ==============================================================
24/01/03 16:24:35 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:24:35 INFO ResourceUtils: ==============================================================
24/01/03 16:24:35 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:24:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:24:35 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:24:35 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:24:36 INFO SecurityManager: Changing view acls to: root
24/01/03 16:24:36 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:24:36 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:24:36 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:24:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:24:40 INFO Utils: Successfully started service 'sparkDriver' on port 34549.
24/01/03 16:24:40 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:24:41 INFO SecurityManager: Changing view acls to: root
24/01/03 16:24:41 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:24:41 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:24:41 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:24:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:24:41 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:24:41 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:24:41 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:24:41 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:24:41 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-d0c246ff-5a79-436b-803f-082b91e90b14
24/01/03 16:24:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:24:41 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:24:41 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:24:42 INFO log: Logging initialized @58596ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:24:43 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:24:44 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:24:44 INFO Server: Started @60255ms
24/01/03 16:24:44 INFO AbstractConnector: Started ServerConnector@19557834{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:24:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/01/03 16:24:45 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43cb23f9{/,null,AVAILABLE,@Spark}
24/01/03 16:24:56 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:24:56 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:24:56 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:24:57 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:24:57 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:24:57 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:24:57 INFO Executor: Java version 1.8.0_382
24/01/03 16:24:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:24:57 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@754fb519 for default.
24/01/03 16:24:57 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:24:57 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:24:57 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:24:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42535.
24/01/03 16:24:57 INFO NettyBlockTransferService: Server created on 10.139.64.4:42535
24/01/03 16:24:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:24:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 42535, None)
24/01/03 16:24:58 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:42535 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 42535, None)
24/01/03 16:24:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 42535, None)
24/01/03 16:24:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 42535, None)
24/01/03 16:25:09 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@43cb23f9{/,null,STOPPED,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@618fe15b{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@41a0cc8b{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1abb20f4{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@78bf7a80{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5de6754a{/stages,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@40355148{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@39ea6e4d{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@446a2521{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3d91063{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@64e34dcd{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1a74c1e0{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3c05bd7a{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7d978177{/storage,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1e82790c{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1f54dcec{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@72516ade{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5e5497b6{/environment,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@69434af8{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@bd94f5{/executors,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@d34086c{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@69cd75d8{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@26e8526e{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7b70da39{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@624eeec{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@a037a2d{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@142ad7c0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@c80c900{/static,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4e8c5eda{/,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4c3bc28d{/api,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43ceffa1{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@16504ec4{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:25:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3720bd42{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:25:45 INFO SecurityManager: Changing view acls to: root
24/01/03 16:25:48 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:25:48 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:25:48 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:25:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:25:49 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:25:49 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:25:49 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:25:49 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:25:49 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:25:49 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:25:51 INFO ResourceUtils: ==============================================================
24/01/03 16:25:51 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:25:51 INFO ResourceUtils: ==============================================================
24/01/03 16:25:51 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:25:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:25:51 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:25:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:25:51 INFO SecurityManager: Changing view acls to: root
24/01/03 16:25:51 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:25:51 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:25:51 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:25:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:25:53 INFO Utils: Successfully started service 'sparkDriver' on port 36975.
24/01/03 16:25:53 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:25:53 INFO SecurityManager: Changing view acls to: root
24/01/03 16:25:53 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:25:53 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:25:53 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:25:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:25:54 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:25:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:25:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:25:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:25:54 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-3aeb6ede-5e20-4d72-8858-8384dd422328
24/01/03 16:25:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:25:54 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:25:54 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:25:55 INFO log: Logging initialized @55389ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:25:56 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:25:56 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:25:56 INFO Server: Started @56418ms
24/01/03 16:25:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
24/01/03 16:25:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
24/01/03 16:25:56 INFO AbstractConnector: Started ServerConnector@41e35b95{HTTP/1.1, (http/1.1)}{10.139.64.4:4042}
24/01/03 16:25:56 INFO Utils: Successfully started service 'SparkUI' on port 4042.
24/01/03 16:25:57 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@59d18330{/,null,AVAILABLE,@Spark}
24/01/03 16:26:03 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:26:03 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:26:03 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:26:04 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:26:05 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:26:05 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:26:05 INFO Executor: Java version 1.8.0_382
24/01/03 16:26:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:26:05 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2ddc8669 for default.
24/01/03 16:26:05 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:26:05 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:26:05 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:26:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42005.
24/01/03 16:26:05 INFO NettyBlockTransferService: Server created on 10.139.64.4:42005
24/01/03 16:26:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:26:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 42005, None)
24/01/03 16:26:05 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:42005 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 42005, None)
24/01/03 16:26:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 42005, None)
24/01/03 16:26:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 42005, None)
24/01/03 16:26:15 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@59d18330{/,null,STOPPED,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@16c8dbd5{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4cb44382{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f9d15c4{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7e5d5706{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6505a654{/stages,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6fee78db{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@10e96268{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@216d7143{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@19ae9fc6{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@77b6d13c{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2242ff26{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@485c389{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@60e1a29{/storage,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@37f4d4e4{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7128f249{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4324095b{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@795b989b{/environment,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7d77f1ee{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2be56a53{/executors,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@25848889{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@56bd02b6{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@47bab393{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7dcc30b0{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@74c01e79{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7acbe337{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@48c675a9{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7b0e7150{/static,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@29748c19{/,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@686f8827{/api,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1dfcdca6{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@31c850b{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:26:15 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2c02a0bc{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:26:40 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:26:40 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:26:41 INFO AbstractConnector: Stopped Spark@19557834{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:26:41 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4040
24/01/03 16:26:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:26:41 INFO MemoryStore: MemoryStore cleared
24/01/03 16:26:41 INFO BlockManager: BlockManager stopped
24/01/03 16:26:41 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:26:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:26:41 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:26:41 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:26:41 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-8ae65ed3-8f2a-4fff-8388-e07092924e29
24/01/03 16:26:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-323769f1-814e-4257-9d66-356934abd16e
24/01/03 16:26:42 ERROR Utils: Process WrappedArray(/bin/sh, -c, df /local_disk0) exited with code 143, and 
24/01/03 16:26:42 ERROR NodeStatusMonitor: Failed to get disk info
org.apache.spark.SparkException: Process WrappedArray(/bin/sh, -c, df /local_disk0) exited with code 143. 
	at org.apache.spark.util.Utils$.executeAndGetOutputInternal(Utils.scala:1516)
	at org.apache.spark.debugger.NodeStatusMonitor.getDiskInfo(NodeStatusMonitor.scala:94)
	at org.apache.spark.debugger.NodeStatusMonitor.monitorDisk(NodeStatusMonitor.scala:162)
	at org.apache.spark.debugger.NodeStatusMonitor.monitor(NodeStatusMonitor.scala:272)
	at org.apache.spark.debugger.NodeStatusMonitor$$anon$1.run(NodeStatusMonitor.scala:298)
	at org.apache.spark.util.threads.SparkThreadLocalClearingRunnable.run(SparkThreadLocalClearingScheduledThreadPoolExecutor.scala:74)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
24/01/03 16:26:42 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-8ae65ed3-8f2a-4fff-8388-e07092924e29/pyspark-ce5b7015-f23f-4d8a-b9d3-c721abb226b1
24/01/03 16:26:57 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:26:57 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:26:57 INFO AbstractConnector: Stopped Spark@4fa97dcd{HTTP/1.1, (http/1.1)}{10.139.64.4:4041}
24/01/03 16:26:57 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4041
24/01/03 16:26:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:26:57 INFO MemoryStore: MemoryStore cleared
24/01/03 16:26:57 INFO BlockManager: BlockManager stopped
24/01/03 16:26:57 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:26:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:26:57 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:26:58 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:26:58 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-14506c96-f3df-4518-94c7-634005892d6b
24/01/03 16:26:58 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-14506c96-f3df-4518-94c7-634005892d6b/pyspark-ccdbc7df-e40e-4803-9667-44167af60b4c
24/01/03 16:26:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-011ef855-6220-40b1-a048-97cf2595a1f2
24/01/03 16:27:37 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:27:37 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:27:38 INFO AbstractConnector: Stopped Spark@41e35b95{HTTP/1.1, (http/1.1)}{10.139.64.4:4042}
24/01/03 16:27:38 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4042
24/01/03 16:27:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:27:38 INFO MemoryStore: MemoryStore cleared
24/01/03 16:27:38 INFO BlockManager: BlockManager stopped
24/01/03 16:27:38 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:27:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:27:38 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:27:38 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:27:38 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-84535553-960f-4be0-a879-604ee4491132
24/01/03 16:27:39 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-84535553-960f-4be0-a879-604ee4491132/pyspark-fbb12f4b-7a11-425b-8f46-3620d8e35a19
24/01/03 16:27:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-8b58a15c-7013-44b6-a54c-195df19c60cb
24/01/03 16:29:15 INFO SecurityManager: Changing view acls to: root
24/01/03 16:29:17 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:29:17 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:29:17 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:29:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:29:18 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:29:18 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:29:18 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:29:18 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:29:18 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:29:18 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:29:19 INFO ResourceUtils: ==============================================================
24/01/03 16:29:19 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:29:19 INFO ResourceUtils: ==============================================================
24/01/03 16:29:19 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:29:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:29:19 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:29:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:29:19 INFO SecurityManager: Changing view acls to: root
24/01/03 16:29:19 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:29:19 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:29:19 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:29:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:29:21 INFO Utils: Successfully started service 'sparkDriver' on port 32943.
24/01/03 16:29:21 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:29:21 INFO SecurityManager: Changing view acls to: root
24/01/03 16:29:21 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:29:21 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:29:21 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:29:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:29:21 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:29:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:29:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:29:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:29:21 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-1ecd1b75-7ea0-478c-9efd-65352acfd074
24/01/03 16:29:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:29:21 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:29:21 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:29:22 INFO log: Logging initialized @31181ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:29:22 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:29:22 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:29:22 INFO Server: Started @31823ms
24/01/03 16:29:23 INFO AbstractConnector: Started ServerConnector@5ec6b793{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:29:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/01/03 16:29:23 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@33ef931d{/,null,AVAILABLE,@Spark}
24/01/03 16:29:25 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:29:25 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:29:25 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:29:25 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:29:26 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:29:26 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:29:26 INFO Executor: Java version 1.8.0_382
24/01/03 16:29:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:29:26 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4188506d for default.
24/01/03 16:29:26 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:29:26 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:29:26 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:29:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40675.
24/01/03 16:29:26 INFO NettyBlockTransferService: Server created on 10.139.64.4:40675
24/01/03 16:29:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:29:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 40675, None)
24/01/03 16:29:26 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:40675 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 40675, None)
24/01/03 16:29:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 40675, None)
24/01/03 16:29:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 40675, None)
24/01/03 16:29:32 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@33ef931d{/,null,STOPPED,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2176a1ae{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@73e5e1{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5acccb84{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6e95a3cb{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@10a6460e{/stages,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d5159d1{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@32b295fa{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3510b172{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@10e8e03f{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@31a05c10{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4d1477e2{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2081aee6{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@ce82f{/storage,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2fa5f0dd{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6faa9ac4{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@19c6ce6a{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2838a436{/environment,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@31453663{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3427002c{/executors,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a82179a{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@70b0d60{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@699b27ac{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3756b33d{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a36585e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6eae0084{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@69cafac0{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7a63f78{/static,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6d1ea65b{/,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1a96300b{/api,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6f278a80{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4d1adb65{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:29:32 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1dfd3335{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:31:14 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:31:14 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:31:14 INFO AbstractConnector: Stopped Spark@5ec6b793{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:31:14 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4040
24/01/03 16:31:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:31:14 INFO MemoryStore: MemoryStore cleared
24/01/03 16:31:14 INFO BlockManager: BlockManager stopped
24/01/03 16:31:14 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:31:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:31:15 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:31:15 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:31:15 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-28e1dc88-8711-4e7a-8a4c-ae0b8b9d795f/pyspark-48f8df2e-290e-4802-871d-5ccb43e6de86
24/01/03 16:31:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-ba29f0e7-badc-4b19-a3bb-d347d7a111ea
24/01/03 16:31:16 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-28e1dc88-8711-4e7a-8a4c-ae0b8b9d795f
24/01/03 16:32:01 INFO SecurityManager: Changing view acls to: root
24/01/03 16:32:03 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:32:03 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:32:03 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:32:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:32:04 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:32:04 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:32:04 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:32:04 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:32:04 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:32:04 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:32:05 INFO ResourceUtils: ==============================================================
24/01/03 16:32:05 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:32:05 INFO ResourceUtils: ==============================================================
24/01/03 16:32:05 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:32:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:32:05 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:32:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:32:05 INFO SecurityManager: Changing view acls to: root
24/01/03 16:32:05 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:32:05 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:32:05 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:32:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:32:07 INFO Utils: Successfully started service 'sparkDriver' on port 44755.
24/01/03 16:32:07 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:32:07 INFO SecurityManager: Changing view acls to: root
24/01/03 16:32:07 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:32:07 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:32:07 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:32:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:32:07 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:32:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:32:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:32:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:32:07 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-993397b9-851c-490f-b2fa-c5fbdd9eb067
24/01/03 16:32:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:32:08 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:32:08 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:32:08 INFO log: Logging initialized @31056ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:32:09 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:32:09 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:32:09 INFO Server: Started @31874ms
24/01/03 16:32:09 INFO AbstractConnector: Started ServerConnector@4bf2d26d{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:32:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/01/03 16:32:09 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3323ba47{/,null,AVAILABLE,@Spark}
24/01/03 16:32:11 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:32:11 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:32:12 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:32:12 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:32:12 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:32:12 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:32:12 INFO Executor: Java version 1.8.0_382
24/01/03 16:32:12 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:32:12 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6e112cbd for default.
24/01/03 16:32:12 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:32:12 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:32:12 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:32:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35723.
24/01/03 16:32:13 INFO NettyBlockTransferService: Server created on 10.139.64.4:35723
24/01/03 16:32:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:32:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 35723, None)
24/01/03 16:32:13 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:35723 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 35723, None)
24/01/03 16:32:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 35723, None)
24/01/03 16:32:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 35723, None)
24/01/03 16:32:18 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@3323ba47{/,null,STOPPED,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5e66a910{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5a9df356{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@64d4ecdd{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@268daac0{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@51b66858{/stages,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7e0675c0{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@787b9033{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@28086cc3{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b56a2ce{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4659a583{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@152aafdf{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@56143424{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5c5b87fe{/storage,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@21443f7d{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@41b02ff6{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@234f15f5{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3d633d7{/environment,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@78f432dc{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4201a417{/executors,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43720c7e{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2706dc9b{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@23a37cbb{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1df31749{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@59e95548{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@572c00a9{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:32:18 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@18ec9290{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:32:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7da77acc{/static,null,AVAILABLE,@Spark}
24/01/03 16:32:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@35a2c68e{/,null,AVAILABLE,@Spark}
24/01/03 16:32:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3b2078b9{/api,null,AVAILABLE,@Spark}
24/01/03 16:32:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@30dec351{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:32:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1792246{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:32:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@721bb495{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:33:35 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:33:35 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:33:35 INFO AbstractConnector: Stopped Spark@4bf2d26d{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:33:35 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4040
24/01/03 16:33:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:33:36 INFO MemoryStore: MemoryStore cleared
24/01/03 16:33:36 INFO BlockManager: BlockManager stopped
24/01/03 16:33:36 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:33:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:33:36 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:33:36 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:33:36 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-e1609f19-815b-46e4-b6bd-497a2d6217f4
24/01/03 16:33:37 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-e1609f19-815b-46e4-b6bd-497a2d6217f4. Falling back to Java IO way
java.io.IOException: Failed to delete: /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-e1609f19-815b-46e4-b6bd-497a2d6217f4
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1278)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2235)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
24/01/03 16:33:37 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-e1609f19-815b-46e4-b6bd-497a2d6217f4/pyspark-96e9f05c-e8c7-432e-8d22-166750d0430d
24/01/03 16:33:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-4b616f5e-63a9-43cc-aebf-5f12ed6929b5
24/01/03 16:33:54 INFO SecurityManager: Changing view acls to: root
24/01/03 16:33:56 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:33:56 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:33:56 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:33:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:33:57 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:33:57 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:33:57 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:33:57 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:33:57 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:33:57 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:33:58 INFO ResourceUtils: ==============================================================
24/01/03 16:33:58 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:33:58 INFO ResourceUtils: ==============================================================
24/01/03 16:33:58 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:33:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:33:58 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:33:58 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:33:59 INFO SecurityManager: Changing view acls to: root
24/01/03 16:33:59 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:33:59 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:33:59 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:33:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:34:00 INFO Utils: Successfully started service 'sparkDriver' on port 34499.
24/01/03 16:34:00 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:34:00 INFO SecurityManager: Changing view acls to: root
24/01/03 16:34:00 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:34:00 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:34:00 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:34:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:34:00 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:34:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:34:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:34:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:34:00 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-1a293702-337c-4766-a61e-aa8cf5322c28
24/01/03 16:34:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:34:01 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:34:01 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:34:01 INFO log: Logging initialized @29082ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:34:02 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:34:02 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:34:02 INFO Server: Started @29762ms
24/01/03 16:34:02 INFO AbstractConnector: Started ServerConnector@60ae82ec{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:34:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/01/03 16:34:02 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4e9c4e43{/,null,AVAILABLE,@Spark}
24/01/03 16:34:04 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:34:04 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:34:04 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:34:05 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:34:05 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:34:05 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:34:05 INFO Executor: Java version 1.8.0_382
24/01/03 16:34:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:34:05 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@412e46e9 for default.
24/01/03 16:34:05 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:34:05 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:34:05 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:34:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39817.
24/01/03 16:34:06 INFO NettyBlockTransferService: Server created on 10.139.64.4:39817
24/01/03 16:34:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:34:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 39817, None)
24/01/03 16:34:06 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:39817 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 39817, None)
24/01/03 16:34:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 39817, None)
24/01/03 16:34:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 39817, None)
24/01/03 16:34:12 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@4e9c4e43{/,null,STOPPED,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1c7d79c5{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4d7a239c{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@32b101a6{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7bc556e2{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@44ee87ef{/stages,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@66e0b94b{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@20039e64{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@19f3fa04{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@348974aa{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@22aca316{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7cc04abc{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@55a68da4{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@32556293{/storage,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@68d6f1e4{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4f2e079f{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7d0f5e5a{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@21869e00{/environment,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a085ad6{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1a80cc18{/executors,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4efd544d{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@445f071d{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7d5741ae{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6686a35{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@79ae9ca2{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1968818e{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@35bbce51{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@17e67690{/static,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7c78a2bf{/,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5d3dfbcf{/api,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7774c951{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@470730db{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:34:12 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2d7df3e5{/metrics/json,null,AVAILABLE,@Spark}
24/01/03 16:35:30 INFO SparkContext: Invoking stop() from shutdown hook
24/01/03 16:35:30 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/01/03 16:35:30 INFO AbstractConnector: Stopped Spark@60ae82ec{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:35:30 INFO SparkUI: Stopped Spark web UI at http://10.139.64.4:4040
24/01/03 16:35:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/01/03 16:35:31 INFO MemoryStore: MemoryStore cleared
24/01/03 16:35:31 INFO BlockManager: BlockManager stopped
24/01/03 16:35:31 INFO BlockManagerMaster: BlockManagerMaster stopped
24/01/03 16:35:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/01/03 16:35:31 INFO SparkContext: Successfully stopped SparkContext
24/01/03 16:35:31 INFO ShutdownHookManager: Shutdown hook called
24/01/03 16:35:31 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-18b46b6e-f107-46f1-9552-6da61275cf4b/pyspark-72a7f755-2782-4949-bd52-78489a5d32e8
24/01/03 16:35:31 INFO ShutdownHookManager: Deleting directory /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/spark-18b46b6e-f107-46f1-9552-6da61275cf4b
24/01/03 16:35:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-4558b4b5-e9d5-4159-ad94-dcf70f59e5aa
24/01/03 16:36:21 INFO SecurityManager: Changing view acls to: root
24/01/03 16:36:24 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:36:24 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:36:24 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:36:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:36:24 INFO SparkContext: Running Spark version 3.5.0
24/01/03 16:36:24 INFO SparkContext: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:36:24 INFO SparkContext: Java version 1.8.0_382
24/01/03 16:36:24 INFO DatabricksEdgeConfigs: serverlessEnabled : false
24/01/03 16:36:24 INFO DatabricksEdgeConfigs: perfPackEnabled : false
24/01/03 16:36:24 INFO DatabricksEdgeConfigs: classicSqlEnabled : false
24/01/03 16:36:25 INFO ResourceUtils: ==============================================================
24/01/03 16:36:25 INFO ResourceUtils: No custom resources configured for spark.driver.
24/01/03 16:36:25 INFO ResourceUtils: ==============================================================
24/01/03 16:36:25 INFO SparkContext: Submitted application: pyspark-shell
24/01/03 16:36:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/01/03 16:36:25 INFO ResourceProfile: Limiting resource is cpu
24/01/03 16:36:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/01/03 16:36:25 INFO SecurityManager: Changing view acls to: root
24/01/03 16:36:25 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:36:25 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:36:25 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:36:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:36:27 INFO Utils: Successfully started service 'sparkDriver' on port 44489.
24/01/03 16:36:27 INFO SparkEnv: Registering MapOutputTracker
24/01/03 16:36:27 INFO SecurityManager: Changing view acls to: root
24/01/03 16:36:27 INFO SecurityManager: Changing modify acls to: root
24/01/03 16:36:27 INFO SecurityManager: Changing view acls groups to: 
24/01/03 16:36:27 INFO SecurityManager: Changing modify acls groups to: 
24/01/03 16:36:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled
24/01/03 16:36:27 INFO SparkEnv: Registering BlockManagerMaster
24/01/03 16:36:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/01/03 16:36:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/01/03 16:36:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/01/03 16:36:27 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-d068d39a-caae-43dd-8a7a-eb42d10a8a4a/blockmgr-3ede5d02-edc8-4ee2-aaed-0ca764b49155
24/01/03 16:36:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/01/03 16:36:27 INFO SparkEnv: Registering OutputCommitCoordinator
24/01/03 16:36:28 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
24/01/03 16:36:28 INFO log: Logging initialized @30963ms to org.eclipse.jetty.util.log.Slf4jLog
24/01/03 16:36:28 INFO JettyUtils: Start Jetty 10.139.64.4:4040 for SparkUI
24/01/03 16:36:29 INFO Server: jetty-9.4.52.v20230823; built: 2023-08-23T19:29:37.669Z; git: abdcda73818a1a2c705da276edb0bf6581e7997e; jvm 1.8.0_382-b05
24/01/03 16:36:29 INFO Server: Started @31664ms
24/01/03 16:36:29 INFO AbstractConnector: Started ServerConnector@4e24469e{HTTP/1.1, (http/1.1)}{10.139.64.4:4040}
24/01/03 16:36:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/01/03 16:36:29 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1905d1f2{/,null,AVAILABLE,@Spark}
24/01/03 16:36:31 ERROR SparkConnectService: Could not start Spark Connect GRPC service
java.io.IOException: Failed to bind to address 0.0.0.0/0.0.0.0:15002
	at grpc_shaded.io.grpc.netty.NettyServer.start(NettyServer.java:328)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:183)
	at grpc_shaded.io.grpc.internal.ServerImpl.start(ServerImpl.java:92)
	at org.apache.spark.sql.connect.service.SparkConnectService$.startGRPCService(SparkConnectService.scala:456)
	at org.apache.spark.sql.connect.service.SparkConnectService$.start(SparkConnectService.scala:462)
	at org.apache.spark.sql.connect.SparkConnectPlugin$$anon$1.init(SparkConnectPlugin.scala:48)
	at org.apache.spark.internal.plugin.DriverPluginContainer.$anonfun$driverPlugins$1(PluginContainer.scala:55)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.internal.plugin.DriverPluginContainer.<init>(PluginContainer.scala:48)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:224)
	at org.apache.spark.internal.plugin.PluginContainer$.apply(PluginContainer.scala:195)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:815)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:60)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)
	at py4j.Gateway.invoke(Gateway.java:257)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:115)
	at java.lang.Thread.run(Thread.java:750)
Caused by: grpc_shaded.io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
24/01/03 16:36:31 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin.
24/01/03 16:36:31 INFO DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger
24/01/03 16:36:32 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:36:32 INFO Executor: Starting executor ID driver on host 10.139.64.4
24/01/03 16:36:32 INFO Executor: OS info Linux, 5.15.0-1051-azure, amd64
24/01/03 16:36:32 INFO Executor: Java version 1.8.0_382
24/01/03 16:36:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/01/03 16:36:32 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3f90b5f0 for default.
24/01/03 16:36:32 INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin.
24/01/03 16:36:32 INFO TaskSchedulerImpl: Preemption disabled in FIFO scheduling mode.
24/01/03 16:36:32 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
24/01/03 16:36:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42701.
24/01/03 16:36:33 INFO NettyBlockTransferService: Server created on 10.139.64.4:42701
24/01/03 16:36:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/01/03 16:36:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.139.64.4, 42701, None)
24/01/03 16:36:33 INFO BlockManagerMasterEndpoint: Registering block manager 10.139.64.4:42701 with 366.3 MiB RAM, BlockManagerId(driver, 10.139.64.4, 42701, None)
24/01/03 16:36:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.139.64.4, 42701, None)
24/01/03 16:36:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.139.64.4, 42701, None)
24/01/03 16:36:40 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@1905d1f2{/,null,STOPPED,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@670ca8d4{/jobs,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@15891048{/jobs/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7ad67e01{/jobs/job,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@65dd5e3c{/jobs/job/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2e940b3e{/stages,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@743ed13{/stages/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@275730a6{/stages/stage,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@308fc111{/stages/stage/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3272b404{/stages/pool,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@142abf05{/stages/pool/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6c278e84{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6cc682cb{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@61c10c1e{/storage,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3a088e4e{/storage/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@751cba6c{/storage/rdd,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6dbe02f7{/storage/rdd/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@462e3068{/environment,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1cc624ea{/environment/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@748ac427{/executors,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@49987254{/executors/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@28c8ded0{/executors/threadDump,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6185fb04{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@9508889{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5f34a4c{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@68ade9a6{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3a111922{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@798c7ff2{/static,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@675f0056{/,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2ccf1697{/api,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@49d82bb{/jobs/job/kill,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@52307d6b{/stages/stage/kill,null,AVAILABLE,@Spark}
24/01/03 16:36:40 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@851d2d7{/metrics/json,null,AVAILABLE,@Spark}
